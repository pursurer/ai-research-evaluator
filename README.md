# AI Research Direction Evaluator

> åŸºäº P-F-C æ¨¡å‹çš„ AI ç ”ç©¶æ–¹å‘è‡ªåŠ¨è¯„ä¼°ç³»ç»Ÿ  
> Automated AI Research Direction Evaluation System based on P-F-C Model

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Tests](https://github.com/pursurer/ai-research-evaluator/actions/workflows/tests.yml/badge.svg)](https://github.com/pursurer/ai-research-evaluator/actions)

---

## Overview / æ¦‚è¿°

### è§£å†³çš„é—®é¢˜ / Problem

AI ç ”ç©¶è€…åœ¨é€‰æ‹©ç ”ç©¶æ–¹å‘æ—¶é¢ä¸´ä¸‰å¤§å›°å¢ƒï¼š

1. **ä¿¡æ¯è¿‡è½½**ï¼šæ¯å¤© arXiv æ•°åç¯‡æ–°è®ºæ–‡ï¼Œé¡¶ä¼šæ•°åƒç¯‡æŠ•ç¨¿ï¼Œæ— æ³•å…¨é¢è¿½è¸ª
2. **åˆ¤æ–­ç›²åŒº**ï¼šä¸æ¸…æ¥šæŸæ–¹å‘æ˜¯"è“æµ·æœºä¼š"è¿˜æ˜¯"å·¨å¤´å„æ–­çš„çº¢æµ·"
3. **èµ„æºé”™é…**ï¼šæŠ•å…¥æ•°æœˆåæ‰å‘ç°ç®—åŠ›ä¸è¶³ã€æ•°æ®éš¾è·å–ï¼Œæ²‰æ²¡æˆæœ¬å·¨å¤§

### ç³»ç»ŸåŠŸèƒ½ / What It Does

**è¾“å…¥ / Input**ï¼š
- ç”¨æˆ·æ‰‹åŠ¨è¾“å…¥ç ”ç©¶æ–¹å‘å…³é”®è¯ï¼ˆå¦‚ "Video Generation"ã€"RAG"ã€"Multimodal Alignment"ï¼‰
- æˆ–ç³»ç»Ÿä»é¡¶ä¼šè®ºæ–‡ä¸­è‡ªåŠ¨å‘ç°å¹¶æ¨èçƒ­ç‚¹æ–¹å‘

**è¾“å‡º / Output**ï¼š
- è¯¥æ–¹å‘çš„ **7 é¡¹æŒ‡æ ‡é‡åŒ–è¯„åˆ†**ï¼ˆ1-10 åˆ†ï¼‰
- **ç»¼åˆ ROI åˆ†æ•°**
- **å†³ç­–å»ºè®®**ï¼ˆ4 çº§åˆ†ç±»ï¼‰
- **è¶‹åŠ¿åˆ†æ**ä¸ **6-12 ä¸ªæœˆæ¼”è¿›é¢„æµ‹**
- **æ½œåœ¨é£é™©æç¤º**
- **æ ¸å¿ƒç«äº‰ç‚¹åˆ†æ**

---

## P-F-C è¯„ä¼°æ¡†æ¶ / Evaluation Framework

### è®¾è®¡ç†å¿µ / Design Philosophy

å€Ÿé‰´æŠ•èµ„é¢†åŸŸçš„"é£é™©æ”¶ç›Šè¯„ä¼°"æ€æƒ³ï¼Œå°†ç ”ç©¶æ–¹å‘è§†ä¸ºä¸€é¡¹"æŠ•èµ„"ï¼š
- **P (Potential)**ï¼šæ½œåœ¨æ”¶ç›Šæœ‰å¤šå¤§ï¼Ÿ
- **F (Feasibility)**ï¼šæˆ‘æœ‰èƒ½åŠ›å®ç°å—ï¼Ÿ
- **C (Competition)**ï¼šç«äº‰å¯¹æ‰‹æ˜¯è°ï¼Ÿ

### P ç»´åº¦ï¼šç§‘ç ”æ½œåŠ› / Potentialï¼ˆæƒé‡ 35%ï¼‰

> å†³å®šäº†è®ºæ–‡æ˜¯å‘åœ¨é¡¶ä¼š Oral è¿˜æ˜¯æ™®é€š Workshop

#### P1 è¶‹åŠ¿çº¢åˆ© / Trend Momentum

**æ ¸å¿ƒé—®é¢˜**ï¼šè¯¥æ–¹å‘åœ¨é¡¶ä¼šçš„çƒ­åº¦è¶‹åŠ¿å¦‚ä½•ï¼Ÿæ˜¯å¤„äºçˆ†å‘æœŸè¿˜æ˜¯è¡°é€€æœŸï¼Ÿ

| è¯„åˆ† | ç‰¹å¾ | å…·ä½“ä¾‹å­ |
|------|------|---------|
| **8-10 åˆ†** | æŒ‡æ•°çˆ†å‘æœŸï¼Œè®ºæ–‡æ•°é‡å¹´å¢é•¿ >50% | **Video Generation**ï¼šNeurIPS 2025 å‡ºç° *Deep Compositional Phase Diffusion for Long Motion Sequence Generation* ç­‰å¤šç¯‡ Oralï¼ŒSora å‘å¸ƒåè¯¥æ–¹å‘è®ºæ–‡æ¿€å¢ |
| **5-7 åˆ†** | ç¨³å®šå¢é•¿ï¼Œä¸»æµä½†éçˆ†å‘ | **Mixture-of-Experts (MoE)**ï¼š*Advancing Expert Specialization for Better MoE*ã€*Gated Attention for LLMs* æŒç»­æœ‰é«˜è´¨é‡å·¥ä½œï¼Œä½†å¢é€Ÿå¹³ç¨³ |
| **1-4 åˆ†** | é€å¹´ä¸‹é™ï¼Œè¢«è§†ä¸ºè¿‡æ—¶ | ä¼ ç»Ÿ CNN å›¾åƒåˆ†ç±»ï¼šè¢« ViT æ›¿ä»£ï¼Œé¡¶ä¼šè®ºæ–‡æ•°é‡é”å‡ |

#### P2 å™äº‹æ·±åº¦ / Narrative Depth

**æ ¸å¿ƒé—®é¢˜**ï¼šè¿™æ˜¯ä¸€ä¸ªçœŸæ­£çš„ç§‘å­¦é—®é¢˜ï¼Œè¿˜æ˜¯ä»…ä»…æ˜¯å·¥ç¨‹ Trickï¼Ÿèƒ½å¦å…³è”ç¬¬ä¸€æ€§åŸç†ï¼Ÿ

| è¯„åˆ† | ç‰¹å¾ | å…·ä½“ä¾‹å­ |
|------|------|---------|
| **8-10 åˆ†** | èƒ½å…³è” Scaling Lawã€ç¬¬ä¸€æ€§åŸç† | **Scaling Depth in RL**ï¼š*1000 Layer Networks for Self-Supervised RL* æ¢ç´¢æ·±åº¦ä¸æ€§èƒ½çš„ Scaling å…³ç³»ï¼Œå…·æœ‰ç†è®ºæ·±åº¦ |
| **5-7 åˆ†** | æœ‰ä¸€å®šç†è®ºè´¡çŒ®ï¼Œä½†æ›´åå·¥ç¨‹ | **RAG ä¼˜åŒ–**ï¼š*Boosting Knowledge Utilization in MLLMs via Adaptive Logits Fusion* è§£å†³å®é™…é—®é¢˜ï¼Œä½†ç†è®ºåˆ›æ–°æœ‰é™ |
| **1-4 åˆ†** | çº¯å·¥ç¨‹ Trickï¼Œä»…å¯¹ç‰¹å®šæ•°æ®é›†æœ‰æ•ˆ | é’ˆå¯¹æŸä¸ª benchmark çš„è¶…å‚è°ƒä¼˜ï¼Œæ— æ³•æ³›åŒ– |

#### P3 SOTA æ¸—é€ç‡ / SOTA Saturation

**æ ¸å¿ƒé—®é¢˜**ï¼šæƒ³è¦åœ¨è¿™ä¸ªé¢†åŸŸè¢«è®¤å¯ï¼Œéœ€è¦æå‡å¤šå°‘æ€§èƒ½ï¼Ÿæ¦œå•æ˜¯å¦å·²ç»é¥±å’Œï¼Ÿ

| è¯„åˆ† | ç‰¹å¾ | å…·ä½“ä¾‹å­ |
|------|------|---------|
| **8-10 åˆ†** | æ–°é¢†åŸŸï¼Œç®€å•æ”¹è¿›å³å¯ SOTA | **3D Vision-Language Navigation**ï¼š*Dynam3D* åœ¨ R2R-CEã€REVERIE-CE ç­‰æ–° benchmark ä¸Šå–å¾— SOTAï¼Œé¢†åŸŸå°šæœªé¥±å’Œ |
| **5-7 åˆ†** | ä¸­ç­‰éš¾åº¦ï¼Œéœ€æ˜¾è‘—åˆ›æ–° | **Offline RL**ï¼š*A Clean Slate for Offline RL* éœ€è¦ç³»ç»Ÿæ€§é‡æ–°è®¾è®¡æ‰èƒ½è¶…è¶Š baseline |
| **1-4 åˆ†** | æ¦œå•é¥±å’Œï¼ˆ99%+ï¼‰ï¼Œæå‡æéš¾ | ImageNet åˆ†ç±»ï¼šTop-1 å·²è¾¾ ~91%ï¼Œæå‡ 0.1% éœ€è¦å·¨å¤§èµ„æº |

---

### F ç»´åº¦ï¼šæŠ€æœ¯å¯è¡Œæ€§ / Feasibilityï¼ˆæƒé‡ 40%ï¼‰â€” âš ï¸ æ ¸å¿ƒé£æ§é¡¹

> å¯¹äºéå·¨å¤´å›¢é˜Ÿï¼Œè¿™æ˜¯èƒ½å¦åšå‡ºæ¥çš„**å†³å®šæ€§å› ç´ **

#### F1 ç®—åŠ›åŒ¹é…åº¦ / Compute Matchï¼ˆâš ï¸ ç”Ÿæ­»çº¿ï¼‰

**æ ¸å¿ƒé—®é¢˜**ï¼šä½ çš„æ˜¾å¡/ç®—åŠ›èƒ½å¦æ”¯æ’‘è¿™ä¸ªæ–¹å‘çš„ä¸»æµå®éªŒï¼Ÿ

| è¯„åˆ† | ç‰¹å¾ | å…·ä½“ä¾‹å­ |
|------|------|---------|
| **8-10 åˆ†** | å•å¡ 4090 / Colab å³å¯ | **Sparse Autoencoders å¯è§£é‡Šæ€§**ï¼š*A is for Absorption: Studying Feature Splitting in SAEs* åˆ†æå·²æœ‰æ¨¡å‹ï¼Œæ— éœ€å¤§è§„æ¨¡è®­ç»ƒ |
| **5-7 åˆ†** | éœ€è¦ 4-8 å¡ A100ï¼Œå¯ç§Ÿç”¨ | **MoE Fine-tuning**ï¼š*Advancing Expert Specialization for Better MoE* éœ€è¦ä¸­ç­‰è§„æ¨¡ç®—åŠ›è¿›è¡Œ SFT |
| **1-4 åˆ†** | éœ€ H100 é›†ç¾¤ä»é›¶é¢„è®­ç»ƒ | **Video Generation é¢„è®­ç»ƒ**ï¼šSora çº§åˆ«æ¨¡å‹éœ€è¦æ•°åƒ GPU å¤©ï¼Œä¸ªäººç ”ç©¶è€…å‡ ä¹ä¸å¯èƒ½å¤ç° |

#### F2 æ•°æ®è·å– / Data Accessibility

**æ ¸å¿ƒé—®é¢˜**ï¼šåšè¿™ä¸ªæ–¹å‘éœ€è¦çš„æ•°æ®ä»å“ªæ¥ï¼Ÿæ˜¯å¦å¯è·å–ï¼Ÿ

| è¯„åˆ† | ç‰¹å¾ | å…·ä½“ä¾‹å­ |
|------|------|---------|
| **8-10 åˆ†** | HuggingFace ç°æˆé«˜è´¨é‡æ•°æ® | **NLP ä»»åŠ¡**ï¼šå¤§é‡å¼€æºæ•°æ®é›†ï¼ˆC4ã€RedPajamaã€OpenWebTextï¼‰å¯ç›´æ¥ä½¿ç”¨ |
| **5-7 åˆ†** | éœ€è¦ä¸€å®šå¤„ç†æˆ–å°è§„æ¨¡æ ‡æ³¨ | **Vision-Language Navigation**ï¼š*Dynam3D* ä½¿ç”¨ R2Rã€REVERIE ç­‰å…¬å¼€æ•°æ®é›†ï¼Œä½†éœ€è¦ 3D ç¯å¢ƒé…ç½® |
| **1-4 åˆ†** | ç§æœ‰æ•°æ®ã€éœ€æ˜‚è´µäººå·¥æ ‡æ³¨ | **åŒ»ç–—å½±åƒåˆ†æ**ï¼šéœ€è¦åŒ»é™¢åˆä½œè·å– HIPAA åˆè§„æ•°æ®ï¼Œæ ‡æ³¨éœ€è¦ä¸“ä¸šåŒ»ç”Ÿ |

#### F3 è¿­ä»£å‘¨æœŸ / Iteration Time

**æ ¸å¿ƒé—®é¢˜**ï¼šè·‘é€šä¸€æ¬¡å®Œæ•´å®éªŒéœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ

| è¯„åˆ† | ç‰¹å¾ | å…·ä½“ä¾‹å­ |
|------|------|---------|
| **8-10 åˆ†** | < 12 å°æ—¶ï¼ˆä¸€å¤©éªŒè¯ 2 ä¸ª Ideaï¼‰ | **SAE åˆ†æ**ï¼šåœ¨å·²æœ‰æ¨¡å‹ä¸Šè¿è¡Œåˆ†æï¼Œå‡ å°æ—¶å‡ºç»“æœ |
| **5-7 åˆ†** | 1-3 å¤© | **RL è®­ç»ƒ**ï¼š*Adaptive Surrogate Gradients for SNN RL* ä¸­ç­‰è§„æ¨¡ RL è®­ç»ƒ |
| **1-4 åˆ†** | > 1 å‘¨ï¼ˆDebug æˆæœ¬æé«˜ï¼‰ | **LLM é¢„è®­ç»ƒ**ï¼šå•æ¬¡è®­ç»ƒæ•°å‘¨ï¼Œå¤±è´¥ä¸€æ¬¡æŸå¤±å·¨å¤§ |

**âš ï¸ å…³é”®è®¾è®¡ï¼šçŸ­æ¿è®¡åˆ†æ³•**
```
F ç»´åº¦å¾—åˆ† = min(F1, F2, F3)
```
> **é€»è¾‘**ï¼šå¦‚æœç®—åŠ›æ˜¯ 2 åˆ†ï¼Œå“ªæ€•æ•°æ®å’Œå‘¨æœŸéƒ½æ˜¯ 10 åˆ†ï¼Œæ•´ä¸ª F é¡¹ä¹Ÿåªèƒ½å¾— 2 åˆ†ã€‚
> 
> **åŸå› **ï¼šå¯è¡Œæ€§æ˜¯"æœ¨æ¡¶æ•ˆåº”"â€”â€”æœ€çŸ­çš„æ¿å†³å®šèƒ½å¦åšå‡ºæ¥ã€‚é¿å…ç ”ç©¶è€…"çœ¼é«˜æ‰‹ä½"ï¼Œé€‰æ‹©è‡ªå·±æ— æ³•å®Œæˆçš„æ–¹å‘ã€‚

---

### C ç»´åº¦ï¼šç«äº‰ç¯å¢ƒ / Competitionï¼ˆæƒé‡ 25%ï¼‰

> å†³å®šäº†æ˜¯å¦ä¼šæ’è½¦åŠç”Ÿå­˜ç©ºé—´

#### C1 å·¨å¤´é¿è®©åº¦ / Giant Avoidanceï¼ˆåå‘æŒ‡æ ‡ï¼‰

**æ ¸å¿ƒé—®é¢˜**ï¼šOpenAIã€Googleã€Metaã€Anthropic æ˜¯å¦åœ¨è¿™ä¸ªæ–¹å‘é‡å…µæŠ•å…¥ï¼Ÿ

| è¯„åˆ† | ç‰¹å¾ | å…·ä½“ä¾‹å­ |
|------|------|---------|
| **8-10 åˆ†** | å†·é—¨æˆ–äº¤å‰é¢†åŸŸï¼Œå¤§å‚æœªæ¶‰è¶³ | **è”é‚¦å­¦ä¹  + å¢é‡å­¦ä¹ **ï¼š*Class-wise Balancing Data Replay for Federated Class-Incremental Learning* äº¤å‰é¢†åŸŸï¼Œå·¨å¤´å…³æ³¨åº¦ä½ |
| **5-7 åˆ†** | å¤§å‚æœ‰å¸ƒå±€ä½†éä¸»æˆ˜åœº | **Offline RL**ï¼šæœ‰ç ”ç©¶ä½†éæˆ˜ç•¥é‡ç‚¹ï¼Œä¸­å°å›¢é˜Ÿä»æœ‰æœºä¼š |
| **1-4 åˆ†** | ä¸»æˆ˜åœºï¼Œæ¯å¤© arXiv å‡ åç¯‡ | **Video Generation**ï¼šSoraã€Veoã€Kling ç­‰å·¨å¤´äº§å“å¯†é›†å‘å¸ƒï¼Œç«äº‰ç™½çƒ­åŒ– |

#### C2 ç ”ç©¶ç©ºç™½åŒº / Research Whitespace

**æ ¸å¿ƒé—®é¢˜**ï¼šè¿™ä¸ªé¢†åŸŸæ˜¯å¦è¿˜æœ‰åˆ›æ–°ç©ºé—´ï¼Ÿèƒ½å¦å®šä¹‰æ–°é—®é¢˜ï¼Ÿ

| è¯„åˆ† | ç‰¹å¾ | å…·ä½“ä¾‹å­ |
|------|------|---------|
| **8-10 åˆ†** | å¤„å¥³åœ°ï¼Œå¯å®šä¹‰æ–°ä»»åŠ¡/æ–° benchmark | **Opinion Intervals in Signed Graphs**ï¼š*Discovering Opinion Intervals from Conflicts* å®šä¹‰å…¨æ–°é—®é¢˜ï¼Œå¼€è¾Ÿæ–°èµ›é“ |
| **5-7 åˆ†** | æœ‰ç©ºé—´ä½†éœ€å·®å¼‚åŒ– | **MoE ä¼˜åŒ–**ï¼šä»æœ‰æ”¹è¿›ç©ºé—´ï¼Œä½†éœ€è¦æ‰¾åˆ°ç‹¬ç‰¹åˆ‡å…¥ç‚¹ï¼ˆå¦‚ *Gated Attention* çš„é—¨æ§æœºåˆ¶ï¼‰ |
| **1-4 åˆ†** | ç¼éš™æå°ï¼Œåªèƒ½åš A+B ç¼åˆ | æˆç†Ÿé¢†åŸŸçš„ incremental improvementï¼Œéš¾ä»¥å‘è¡¨ |

### è¯„åˆ†è®¡ç®— / Scoring Formula

```
ROI Score = 0.35 Ã— P_avg + 0.40 Ã— F_min + 0.25 Ã— C_avg

å…¶ä¸­ï¼š
- P_avg = (P1 + P2 + P3) / 3
- F_min = min(F1, F2, F3)  â† çŸ­æ¿å†³å®š
- C_avg = (C1 + C2) / 2
```

**ğŸš¨ ç†”æ–­æœºåˆ¶**ï¼šè‹¥ `F_min < 3` â†’ æ— è®ºæ€»åˆ†å¤šé«˜ï¼Œç›´æ¥åˆ¤å®š"ä¸å¯è¡Œ"

### å†³ç­–è¾“å‡º / Decision Outputï¼ˆ4 çº§åˆ†ç±»ï¼‰

| ç­‰çº§ | è§¦å‘æ¡ä»¶ | å…¸å‹ç‰¹å¾ | å»ºè®® |
|------|---------|---------|------|
| **æˆ˜ç•¥é‡ç‚¹** | ROI â‰¥ 7.0 | é«˜æ½œæ˜“å‘ï¼šçƒ­åº¦å¤Ÿã€èµ„æºå¤Ÿã€èƒ½è½åœ° | All-inï¼Œè¿½æ±‚é€Ÿåº¦ |
| **å·®å¼‚åŒ–çªå›´** | 5.5 â‰¤ ROI < 7.0 ä¸” P_avg > 8 | é«˜æ½œé«˜éš¾ï¼šæ½œåŠ›æå¤§ä½†æœ‰å•ä¸€çŸ­æ¿ | æ‰¾åˆä½œè€…è¡¥çŸ­æ¿ï¼Œä¸æ­»ç£• |
| **å¿«é€Ÿæ¡æ¼** | 5.5 â‰¤ ROI < 7.0 ä¸” F_min > 8 | ä½æ½œæ˜“è¡Œï¼šå®¹æ˜“åšä½†åˆ›æ–°æœ‰é™ | ä¿åº•ç­–ç•¥ï¼Œé€‚åˆç»ƒæ‰‹ |
| **å®¡æ…é¿å¼€** | ROI < 5.5 æˆ–è§¦å‘ç†”æ–­ | çº¢æµ·æˆ–æ­»èƒ¡åŒ | åšå†³æ”¾å¼ƒï¼Œä¸è¢«åª’ä½“çƒ­åº¦è¿·æƒ‘ |

---

## ä¿¡æ¯æ¥æº / Data Sources

### é™æ€æ•°æ®ï¼šé¡¶ä¼šè®ºæ–‡ / Conference Papersï¼ˆå½“å‰å·²é›†æˆï¼‰

ä» **9 ä¸ª AI é¡¶çº§å­¦æœ¯ä¼šè®®** æå–è®ºæ–‡æ•°æ®ï¼š

| ä¼šè®® | é¢†åŸŸä¾§é‡ |
|------|---------|
| **NeurIPS** | æœºå™¨å­¦ä¹ ç†è®ºä¸åº”ç”¨ |
| **ICLR** | è¡¨ç¤ºå­¦ä¹ ã€æ·±åº¦å­¦ä¹  |
| **ICML** | æœºå™¨å­¦ä¹ æ ¸å¿ƒç®—æ³• |
| **AAAI** | äººå·¥æ™ºèƒ½ç»¼åˆ |
| **ACL** | è‡ªç„¶è¯­è¨€å¤„ç† |
| **EMNLP** | NLP å®è¯æ–¹æ³• |
| **NAACL** | åŒ—ç¾ NLP |
| **IJCAI** | AI ç»¼åˆï¼ˆååº”ç”¨ï¼‰ |
| **AISTATS** | AI ä¸ç»Ÿè®¡äº¤å‰ |

**æå–å­—æ®µ**ï¼šè®ºæ–‡ IDã€æ ‡é¢˜ã€**å…³é”®è¯**ï¼ˆè¶‹åŠ¿åˆ†æï¼‰ã€**æ‘˜è¦**ï¼ˆLLM åˆ†æï¼‰ã€å¹´ä»½ã€å±•ç¤ºç±»å‹

### åŠ¨æ€æƒ…æŠ¥ï¼šè¡Œä¸šåŠ¨æ€ / Industry Intelligenceï¼ˆæœªæ¥æ¨¡å—ï¼‰

**ç›‘æ§å¯¹è±¡**ï¼šOpenAIã€DeepMindã€Meta AIã€Anthropic

| æ¥æº | æƒé‡ | ç†ç”± |
|------|------|------|
| **arXiv é¢„å°æœ¬** | 40% | æŠ€æœ¯æ·±åº¦æœ€é«˜ï¼Œå­¦æœ¯ä¸€æ‰‹èµ„æ–™ |
| **å®˜æ–¹ Blog** | 35% | ä»£è¡¨å…¬å¸æˆ˜ç•¥æ–¹å‘ï¼Œä¿¡å·æ˜ç¡® |
| **X (Twitter)** | 25% | æ—¶æ•ˆæ€§å¼ºä½†å™ªå£°è¾ƒé«˜ |

**æ›´æ–°é¢‘ç‡**ï¼šæ¯å‘¨æ±‡æ€»

### å¤–éƒ¨ API å¢å¼º / External APIsï¼ˆæœªæ¥æ¨¡å—ï¼‰

| API | ç”¨é€” |
|-----|------|
| **Google Trends** | è¡¥å…… P1 è¶‹åŠ¿çº¢åˆ©çš„å¤–éƒ¨æ•°æ® |
| **Papers With Code** | è·å– SOTA æ¦œå•ï¼Œé‡åŒ– P3 æŒ‡æ ‡ |
| **Semantic Scholar / OpenAlex** | è¡¥å…¨è®ºæ–‡å¼•ç”¨æ•°ï¼Œè¡¡é‡å½±å“åŠ› |

### å®ä½“å¯¹é½ / Entity Alignmentï¼ˆåŠ¨æ€ â†” é™æ€æ•°æ®å…³è”ï¼‰

| å±‚çº§ | å¯¹é½æ–¹å¼ | ç¤ºä¾‹ |
|------|---------|------|
| **Layer 1** | å…³é”®è¯/ä¸»é¢˜åŒ¹é… | "Video Generation" â†” å«è¯¥å…³é”®è¯çš„è®ºæ–‡ |
| **Layer 2** | æ–¹æ³•/æ¨¡å‹å¯¹é½ | "Sora" â†” Video Diffusion ç›¸å…³è®ºæ–‡ |
| **Layer 3** | æœºæ„/ä½œè€…å¯¹é½ | OpenAI åŠ¨æ€ â†” OpenAI ä½œè€…çš„é¡¶ä¼šè®ºæ–‡ |

---

## Installation / å®‰è£…

```bash
# Clone the repository
git clone https://github.com/pursurer/ai-research-evaluator.git
cd ai-research-evaluator

# Create virtual environment
python -m venv venv
source venv/bin/activate  # macOS/Linux
# or: venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Configure environment variables
cp .env.example .env
# Edit .env and fill in your API keys
```

---

## Quick Start / å¿«é€Ÿå¼€å§‹

```bash
# Evaluate a research direction
python scripts/evaluate.py --direction "Video Generation"

# Discover hot research topics
python scripts/discover.py --top-k 10

# Batch evaluation
python scripts/batch_evaluate.py --input directions.txt
```

### Python API

```python
from src.evaluation import EvaluationEngine
from src.llm import LLMClientFactory

# Initialize
llm = LLMClientFactory.create("openai")
engine = EvaluationEngine(llm=llm)

# Evaluate
result = engine.evaluate(direction="Multimodal Alignment")
print(f"ROI Score: {result.roi_score}")
print(f"Decision: {result.decision.value}")
```

---

## Project Structure / é¡¹ç›®ç»“æ„

```
ai-research-evaluator/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/           # æ•°æ®åŠ è½½ä¸å¤„ç†
â”‚   â”œâ”€â”€ evaluation/     # P-F-C è¯„ä¼°å¼•æ“
â”‚   â”œâ”€â”€ llm/            # LLM æŠ½è±¡å±‚
â”‚   â”œâ”€â”€ report/         # æŠ¥å‘Šç”Ÿæˆ
â”‚   â””â”€â”€ utils/          # é€šç”¨å·¥å…·
â”œâ”€â”€ tests/              # æµ‹è¯•ç”¨ä¾‹
â”œâ”€â”€ scripts/            # CLI è„šæœ¬
â”œâ”€â”€ config/             # é…ç½®æ–‡ä»¶
â””â”€â”€ reports/            # ç”Ÿæˆçš„æŠ¥å‘Š
```

---

## Supported Data Sources / æ”¯æŒçš„æ•°æ®æº

### Conference Papers / é¡¶ä¼šè®ºæ–‡ (å·²é›†æˆ)
| Conference | Status |
|------------|--------|
| NeurIPS | âœ… |
| ICLR | âœ… |
| ICML | âœ… |
| AAAI | âœ… |
| ACL | âœ… |
| EMNLP | âœ… |
| NAACL | âœ… |
| IJCAI | âœ… |
| AISTATS | âœ… |

---

## Roadmap / æœªæ¥æ¨¡å—

> ä»¥ä¸‹åŠŸèƒ½å°†åœ¨åç»­ç‰ˆæœ¬ä¸­é€æ­¥å®ç°

| Module | Description | Priority |
|--------|-------------|----------|
| **External Trends** | Google Trends API é›†æˆ | P1 |
| **Papers With Code** | SOTA æ¦œå•æ•°æ®è·å– | P1 |
| **Citation Data** | Semantic Scholar / OpenAlex API | P2 |
| **X (Twitter) Monitor** | å¤§å‚å®˜æ–¹åŠ¨æ€ç›‘æ§ | P3 |
| **Blog RSS** | AI å®éªŒå®¤æŠ€æœ¯åšå®¢è®¢é˜… | P3 |
| **arXiv Tracker** | é¢„å°æœ¬è®ºæ–‡è¿½è¸ª | P3 |
| **Entity Alignment** | åŠ¨æ€æƒ…æŠ¥ä¸è®ºæ–‡æ•°æ®å¯¹é½ | P4 |

---

## Configuration / é…ç½®

### Environment Variables / ç¯å¢ƒå˜é‡

```bash
# LLM Provider (choose one)
LLM_PROVIDER=openai          # openai / anthropic / local
OPENAI_API_KEY=sk-xxx
ANTHROPIC_API_KEY=sk-ant-xxx

# Data Path
PAPERS_DATA_ROOT=/path/to/é¡¶ä¼šè®ºæ–‡ä¿¡æ¯

# GitHub (for auto-sync)
GITHUB_TOKEN=ghp_xxx
```

### LLM Support / æ”¯æŒçš„ LLM

- OpenAI GPT-4 / GPT-4o
- Anthropic Claude 3.5
- Local models (Ollama, etc.)

---

## Development / å¼€å‘

```bash
# Run tests
pytest

# Code formatting
black src/ tests/
isort src/ tests/

# Type checking
mypy src/

# Lint
ruff check src/
```

---

## License / è®¸å¯è¯

MIT License - see [LICENSE](LICENSE) for details.

---

## Contributing / è´¡çŒ®

1. Fork the repository
2. Create your feature branch (`git checkout -b feat/amazing-feature`)
3. Commit your changes (`git commit -m 'feat: add amazing feature'`)
4. Push to the branch (`git push origin feat/amazing-feature`)
5. Open a Pull Request
